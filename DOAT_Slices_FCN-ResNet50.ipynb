{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOTA 使用FCN做15类的异常检测, ResNet50对error_map分类\n",
    "\n",
    "## 输出3*15通道 [x0,x1,...,x14], 查看wx 与  [x0,x1,...,x14]之间的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = r\"D:/datasets/ShipVSField\"\n",
    "data_dir = r\"D:\\datasets\\dota\\Slices\"\n",
    "data_transform = {x:transforms.Compose([transforms.Resize([256,256]),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)])\n",
    "                                      for x in [\"train\"] } #, \"valid\"\n",
    "\n",
    "# 删除隐藏文件夹 .ipynb_checkpoints\n",
    "for x in [\"train\", \"valid\"]:\n",
    "    if os.path.exists( os.path.join(data_dir,x,\".ipynb_checkpoints\") ):\n",
    "        print(u\"Delete \".format(os.path.join(data_dir,x,\".ipynb_checkpoints\") ))\n",
    "        os.removedirs(os.path.join(data_dir,x,\".ipynb_checkpoints\"))\n",
    "    \n",
    "image_datasets = {x:datasets.ImageFolder(root = os.path.join(data_dir,x),\n",
    "                                        transform = data_transform[x]) for x in [\"train\"]} #, \"valid\"\n",
    "\n",
    "dataloader = {x:torch.utils.data.DataLoader(dataset = image_datasets[x],\n",
    "                                           batch_size = 8, #16,\n",
    "                                           shuffle = True) for x in [\"train\"]}#, \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseball-diamond': 0, 'basketball-court': 1, 'bridge': 2, 'ground-track-field': 3, 'harbor': 4, 'helicopter': 5, 'large-vehicle': 6, 'plane': 7, 'roundabout': 8, 'ship': 9, 'small-vehicle': 10, 'soccer-ball-field': 11, 'storage-tank': 12, 'swimming-pool': 13, 'tennis-court': 14}\n"
     ]
    }
   ],
   "source": [
    "X_example,y_example = next(iter( dataloader[\"train\"] ) )\n",
    "example_classes = image_datasets[\"train\"].classes\n",
    "index_classes = image_datasets[\"train\"].class_to_idx\n",
    "cls_num = len(index_classes)\n",
    "\n",
    "print(index_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN_resnet50\n",
    "num_classes = 3 * 15 # 15类\n",
    "model_fcn = torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 分类\n",
    "model_res = torchvision.models.resnet50(pretrained= False)\n",
    "state_dict = torch.load(\"resnet50-19c8e357.pth\")\n",
    "model_res.load_state_dict(state_dict)\n",
    "\n",
    "# 修改输入输出通道数\n",
    "model_res.fc = torch.nn.Linear(2048, 15)\n",
    "model_res.conv1 = torch.nn.Conv2d(3*15,64, kernel_size=7,padding=3,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型参数\n",
    "model_res.load_state_dict(torch.load('DOAT_Slices_FCN-ResNet50_model_last.pth')['model_res'])\n",
    "model_fcn.load_state_dict(torch.load('DOAT_Slices_FCN-ResNet50_model_last.pth')['model_fcn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 45, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(216914.3750, grad_fn=<SumBackward0>),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fcn = model_fcn(X_example )['out'] # * masked_gaussian\n",
    "print(y_fcn.shape)\n",
    "\n",
    "cls_num = len(index_classes)\n",
    "\n",
    "Classes = np.random.randint(low=0, high=cls_num, size=len(X_example))\n",
    "y_class = torch.cat( [y_fcn[i:i+1,np.array(range(0,3))+3*Classes[i],:,:] for i in range(len(Classes))], axis=0)\n",
    "\n",
    "torch.pow(X_example - y_class, 2).sum(), \n",
    "\n",
    "#torch.tensor(X_example.shape).prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 45, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_map = torch.pow(torch.cat([X_example]*cls_num, axis=1) - y_fcn, 2)\n",
    "print(error_map.shape)\n",
    "y_res = model_res(error_map)\n",
    "y_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失和优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SquaredErrorLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, X, Y, Classes):\n",
    "        input_shape = torch.tensor(X.size())\n",
    "        n = X.shape[1]\n",
    "        index = torch.tensor((range(0,n))).to(Classes.device)\n",
    "        Y_class = torch.cat( [Y[i:i+1,index+n*Classes[i],:,:] for i in range(len(Classes))], axis=0)\n",
    "        Error = torch.pow(X - Y_class, 2).sum() / input_shape.prod()\n",
    "        return Error\n",
    "    \n",
    "# FCN 损失和优化函数\n",
    "loss_fcn = SquaredErrorLoss()\n",
    "\n",
    "# ResNet损失和优化函数\n",
    "loss_res = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "import itertools\n",
    "optimizer = torch.optim.Adam(itertools.chain(model_fcn.parameters(),model_res.parameters()), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6,  9, 11,  4,  2, 11,  9,  8], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0628, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classes = torch.tensor( np.random.randint(low=0, high=cls_num, size=len(X_example)) )\n",
    "print(Classes)\n",
    "\n",
    "loss_fcn(X_example,y_fcn, Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置GPU，开始进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del X_example\n",
    "    del y\n",
    "    del y_class\n",
    "    del error_map\n",
    "except:\n",
    "    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)\n",
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# 配置GPU，开始进行训练\n",
    "Use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if Use_gpu:\n",
    "    print( torch.cuda.get_device_properties(device = 0) )\n",
    "    model_fcn = model_fcn.cuda()\n",
    "    model_res = model_res.cuda()\n",
    "    \n",
    "# 配置多GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_fcn = torch.nn.DataParallel(model_fcn)\n",
    "    model_res = torch.nn.DataParallel(model_res)\n",
    "    \n",
    "masked_gaussian = masked_gaussian.to(next(model_fcn.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Train Loss:0.0466, Train ACC:99.3000\n",
      "Current time 6.97 min\n",
      "Batch 1000, Train Loss:0.0454, Train ACC:99.3000\n",
      "Current time 13.87 min\n",
      "Batch 1500, Train Loss:0.0450, Train ACC:99.4083\n",
      "Current time 20.72 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9016601c1b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mrunning_corrects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#组合为一个索引序列\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mUse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m    150\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_n = 100\n",
    "time_open = time.time()\n",
    "f = open('Loss_log.txt', 'a+')\n",
    "f.write(\"\\nDOAT_Slices_FCN begin\\n\")\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    print(\"Epoch {}/{}\".format(epoch, epoch_n - 1))\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    for phase in [\"train\"]: # \"valid\"\n",
    "        if phase == \"train\":\n",
    "            print(\"Training...\")\n",
    "            model_fcn.train(True)\n",
    "            model_res.train(True)\n",
    "        else:\n",
    "            print(\"Valid...\")\n",
    "            model_fcn.train(False)\n",
    "            model_res.train(False) # 把BatchNormalization和DropOut固定住，不会取平均，而是用训练好的值\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        \n",
    "        for batch, data in enumerate(dataloader[phase], start = 1): #组合为一个索引序列\n",
    "            X, y = data\n",
    "            if Use_gpu:\n",
    "                X, y = Variable(X.cuda()), Variable(y.cuda())\n",
    "            else:\n",
    "                X, y = Variable(X), Variable(y)\n",
    "            \n",
    "            # 添加高斯掩膜\n",
    "            y_fcn = model_fcn.forward(X * masked_gaussian.to(X.device))['out']\n",
    "            \n",
    "            error_map = torch.pow(torch.cat([X]*cls_num, axis=1) - y_fcn, 2)\n",
    "            y_pred = model_res.forward(error_map)\n",
    "            \n",
    "            \n",
    "            _, pred = torch.max(y_pred.data, 1)\n",
    "            \n",
    "            loss = loss_fcn(X, y_fcn, y) + loss_res(y_pred, y)\n",
    "            \n",
    "            # 对参数梯度的归零\n",
    "            optimizer.zero_grad()\n",
    "            if phase == \"train\":\n",
    "                # 计算反向传播梯度值 P167\n",
    "                loss.backward() \n",
    "                # 对节点的参数进行梯度更新 P169\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += float(loss.data)\n",
    "            running_corrects += float(torch.sum(pred == y.data))\n",
    "            \n",
    "            if batch%500 == 0 and phase == \"train\":\n",
    "                print(\"Batch {}, Train Loss:{:.4f}, Train ACC:{:.4f}\"\n",
    "                      .format(batch, running_loss/batch, 100*running_corrects/batch/len(pred)))\n",
    "                print(\"Current time {:.2f} min\".format((time.time() - time_open)/60))\n",
    "            \n",
    "        epoch_loss = running_loss*16/len(image_datasets[phase])\n",
    "        epoch_acc = 100*running_corrects/len(image_datasets[phase])\n",
    "        \n",
    "        print(\"{} Loss:{:.4f}  Acc:{:.4f}%\".format(phase, epoch_loss, epoch_acc))\n",
    "        time_end = time.time() - time_open\n",
    "        print(\"Finish time {:.2f} min\".format(time_end/60))\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            torch.save({'model_res':model_res.module.state_dict(),\n",
    "                        'model_fcn':model_fcn.module.state_dict()},\n",
    "                        'DOAT_Slices_FCN-ResNet50_epoch_{}.pth'.format(epoch))\n",
    "        else:\n",
    "            torch.save({'model_res':model_res.state_dict(), \n",
    "                        'model_fcn':model_fcn.state_dict()},\n",
    "                        'DOAT_Slices_FCN-ResNet50_epoch_{}.pth'.format(epoch))\n",
    "    \n",
    "        \n",
    "# 清除显卡缓存\n",
    "torch.cuda.empty_cache()\n",
    "f.close()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.save({'model_res':model_res.module.state_dict(),\n",
    "                'model_fcn':model_fcn.module.state_dict()},\n",
    "                'DOAT_Slices_FCN-ResNet50_model_last.pth')\n",
    "else:\n",
    "    torch.save({'model_res':model_res.state_dict(), \n",
    "                'model_fcn':model_fcn.state_dict()},\n",
    "                'DOAT_Slices_FCN-ResNet50_model_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de4b650ece4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'phase' is not defined"
     ]
    }
   ],
   "source": [
    "phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.save({'model_res':model_res.module.state_dict(),\n",
    "                'model_fcn':model_fcn.module.state_dict()},\n",
    "                'DOAT_Slices_FCN-ResNet50_model_last.pth')\n",
    "else:\n",
    "    torch.save({'model_res':model_res.state_dict(), \n",
    "                'model_fcn':model_fcn.state_dict()},\n",
    "                'DOAT_Slices_FCN-ResNet50_model_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "休眠 2022-07-26 17:52:10.801712\n",
      "休眠 2022-07-27 11:04:16.536572\n"
     ]
    }
   ],
   "source": [
    "print(\"休眠\",datetime.datetime.now())\n",
    "os.system(\"rundll32.exe powrprof.dll,SetSuspendState 0,1,0\")\n",
    "os.system(\"rundll32.exe powrprof.dll,SetSuspendState\")\n",
    "print(\"休眠\",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)\n",
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# 配置GPU，开始进行训练\n",
    "Use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if Use_gpu:\n",
    "    print( torch.cuda.get_device_properties(device = 0) )\n",
    "    model_fcn = model_fcn.cuda()\n",
    "    model_res = model_res.cuda()\n",
    "    \n",
    "# 配置多GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_fcn = torch.nn.DataParallel(model_fcn)\n",
    "    model_res = torch.nn.DataParallel(model_res)\n",
    "    \n",
    "masked_gaussian = masked_gaussian.to(next(model_fcn.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid...\n",
      "Batch 500, Train Loss:0.0621, Train ACC:99.1500\n",
      "Current time 2.85 min\n",
      "Batch 1000, Train Loss:0.0625, Train ACC:99.2625\n",
      "Current time 5.69 min\n",
      "Batch 1500, Train Loss:0.0587, Train ACC:99.2750\n",
      "Current time 8.50 min\n",
      "Batch 2000, Train Loss:0.0583, Train ACC:99.2438\n",
      "Current time 11.30 min\n",
      "Batch 2500, Train Loss:0.0573, Train ACC:99.2550\n",
      "Current time 14.06 min\n",
      "Batch 3000, Train Loss:0.0565, Train ACC:99.2750\n",
      "Current time 16.83 min\n",
      "Batch 3500, Train Loss:0.0576, Train ACC:99.2393\n",
      "Current time 19.58 min\n",
      "Batch 4000, Train Loss:0.0570, Train ACC:99.2313\n",
      "Current time 22.29 min\n",
      "Batch 4500, Train Loss:0.0557, Train ACC:99.2500\n",
      "Current time 25.01 min\n",
      "Batch 5000, Train Loss:0.0553, Train ACC:99.2500\n",
      "Current time 27.70 min\n",
      "Batch 5500, Train Loss:0.0575, Train ACC:99.2477\n",
      "Current time 30.40 min\n",
      "Batch 6000, Train Loss:0.0570, Train ACC:99.2646\n",
      "Current time 33.07 min\n",
      "Batch 6500, Train Loss:0.0580, Train ACC:99.2558\n",
      "Current time 35.73 min\n",
      "Batch 7000, Train Loss:0.0579, Train ACC:99.2607\n",
      "Current time 38.38 min\n",
      "Batch 7500, Train Loss:0.0581, Train ACC:99.2583\n",
      "Current time 41.06 min\n",
      "Batch 8000, Train Loss:0.0582, Train ACC:99.2625\n",
      "Current time 43.68 min\n",
      "Batch 8500, Train Loss:0.0583, Train ACC:99.2515\n",
      "Current time 46.31 min\n",
      "Batch 9000, Train Loss:0.0587, Train ACC:99.2375\n",
      "Current time 48.92 min\n",
      "Batch 9500, Train Loss:0.0586, Train ACC:99.2382\n",
      "Current time 51.55 min\n",
      "Batch 10000, Train Loss:0.0583, Train ACC:99.2425\n",
      "Current time 54.19 min\n",
      "Batch 10500, Train Loss:0.0583, Train ACC:99.2452\n",
      "Current time 56.81 min\n",
      "Batch 11000, Train Loss:0.0585, Train ACC:99.2398\n",
      "Current time 59.43 min\n",
      "Batch 11500, Train Loss:0.0586, Train ACC:99.2402\n",
      "Current time 62.08 min\n",
      "train Loss:0.1176  Acc:99.2371%\n",
      "Finish time 64.33 min\n"
     ]
    }
   ],
   "source": [
    "time_open = time.time()\n",
    "print(\"Valid...\")\n",
    "\n",
    "phase = \"train\"\n",
    "\n",
    "model_fcn.train(False) # 把BatchNormalization和DropOut固定住，不会取平均，而是用训练好的值\n",
    "model_res.train(False)         \n",
    "    \n",
    "running_loss = 0.0\n",
    "running_corrects = 0.0\n",
    "predict = [[] for i in range(len(example_classes))]\n",
    "\n",
    "for batch, data in enumerate(dataloader[phase], start = 1): #组合为一个索引序列\n",
    "    X, y = data\n",
    "    if Use_gpu:\n",
    "        X, y = Variable(X.cuda()), Variable(y.cuda())\n",
    "    else:\n",
    "        X, y = Variable(X), Variable(y)\n",
    "\n",
    "    # 添加高斯掩膜\n",
    "    y_fcn = model_fcn.forward(X * masked_gaussian.to(X.device))['out']\n",
    "\n",
    "    error_map = torch.pow(torch.cat([X]*cls_num, axis=1) - y_fcn, 2)\n",
    "    y_pred = model_res.forward(error_map)\n",
    "    \n",
    "    _, pred = torch.max(y_pred.data, 1)\n",
    "\n",
    "    loss = loss_fcn(X, y_fcn, y) + loss_res(y_pred, y)\n",
    "    \n",
    "    # 记录预测结果\n",
    "    for i in range(len(y)):\n",
    "        predict[y[i]].append( y_pred[i].cpu().detach().numpy() )  #.argmax(dim=1)\n",
    "    \n",
    "    running_loss += float(loss.data)\n",
    "    running_corrects += float(torch.sum(pred == y.data))\n",
    "\n",
    "    if batch%500 == 0:# and phase == \"train\":\n",
    "        print(\"Batch {}, Train Loss:{:.4f}, Train ACC:{:.4f}\"\n",
    "              .format(batch, running_loss/batch, 100*running_corrects/batch/len(pred)))\n",
    "        print(\"Current time {:.2f} min\".format((time.time() - time_open)/60))\n",
    "\n",
    "epoch_loss = running_loss*16/len(image_datasets[phase])\n",
    "epoch_acc = 100*running_corrects/len(image_datasets[phase])\n",
    "\n",
    "print(\"{} Loss:{:.4f}  Acc:{:.4f}%\".format(phase, epoch_loss, epoch_acc))\n",
    "time_end = time.time() - time_open\n",
    "print(\"Finish time {:.2f} min\".format(time_end/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball-diamond : 94.4578313253012%, Num : 415\n",
      "basketball-court : 96.31067961165049%, Num : 515\n",
      "bridge : 99.62765957446808%, Num : 1880\n",
      "ground-track-field : 98.15384615384616%, Num : 325\n",
      "harbor : 98.54320160750167%, Num : 5972\n",
      "helicopter : 96.66136724960255%, Num : 629\n",
      "large-vehicle : 99.80765763058244%, Num : 16637\n",
      "plane : 99.62311557788944%, Num : 7960\n",
      "roundabout : 98.74371859296483%, Num : 398\n",
      "ship : 99.48390450376861%, Num : 26933\n",
      "small-vehicle : 99.13134400386069%, Num : 24866\n",
      "soccer-ball-field : 86.29283489096574%, Num : 321\n",
      "storage-tank : 98.50679741475373%, Num : 4487\n",
      "swimming-pool : 99.71114962449451%, Num : 1731\n",
      "tennis-court : 98.856416772554%, Num : 2361\n"
     ]
    }
   ],
   "source": [
    "pred_vec = [np.array([]) for i in range(len(example_classes))]\n",
    "Correct,Num = 0,0\n",
    "for cat in range(len(predict)):\n",
    "    catname = list(index_classes.keys())[list(index_classes.values()).index(cat)]\n",
    "    rate = np.sum(np.array([p.argmax() for p in predict[cat]]) == cat)/ len(predict[cat])\n",
    "    print(\"{} : {}%, Num : {}\".format(catname, rate*100, len(predict[cat])))\n",
    "    \n",
    "    Correct =  Correct + np.sum(np.array([p.argmax() for p in predict[cat]]) == cat)\n",
    "    Num = Num + len(predict[cat])\n",
    "    \n",
    "    pred_vec[cat] = np.sum( predict[cat], axis=0 ) / len(predict[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(pred_vec).to_csv(\"pred_vec_FCN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
